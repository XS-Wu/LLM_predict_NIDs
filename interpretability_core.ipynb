{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ce0010",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Model interpretability via attention attribution (post-hoc, no retraining).\n",
    "\n",
    "For a (fine-tuned) task-specific model:\n",
    "1) Extract the final-layer self-attention matrix during inference.\n",
    "2) Average attention across heads.\n",
    "3) Use last-token attention (attention distribution of the final query token).\n",
    "4) Normalize token attention weights so they sum to one over the input sequence.\n",
    "5) Aggregate token-level attention into three semantically defined prompt components:\n",
    "   - Date   : year+month\n",
    "   - Disease\n",
    "   - Outcome: cases vs deaths\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "try:\n",
    "    from peft import PeftModel\n",
    "except Exception:\n",
    "    PeftModel = None\n",
    "\n",
    "\n",
    "# =============================\n",
    "# Settings (keep key parameters unchanged)\n",
    "# =============================\n",
    "BASE_MODEL_PATH = \"path/to/your/base_model\"        \n",
    "LORA_ADAPTER_PATH = None                           \n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "MAX_LEN = 128\n",
    "\n",
    "# Attention mode (no retraining)\n",
    "# \"last\"  : last token row (recommended)\n",
    "# \"lastk\" : mean of last K rows\n",
    "# \"mean\"  : mean over all query rows (legacy robustness check)\n",
    "QUERY_MODE = \"last\"\n",
    "LAST_K = 4\n",
    "\n",
    "DEMO_PROMPTS = [\n",
    "    \"2009年1月 肺结核 发病数\",\n",
    "    \"2016-07 Influenza-associated pediatric mortality deaths\",\n",
    "    \"2020/12 Dengue Fever cases\",\n",
    "]\n",
    "\n",
    "\n",
    "# =============================\n",
    "# Attention backend: force eager \n",
    "# =============================\n",
    "def disable_sdpa_flash() -> None:\n",
    "    try:\n",
    "        torch.backends.cuda.enable_flash_sdp(False)\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        torch.backends.cuda.enable_mem_efficient_sdp(False)\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        torch.backends.cuda.enable_math_sdp(True)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "\n",
    "def load_tokenizer_fast_or_fail(model_path: str):\n",
    "    tok = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True, use_fast=True)\n",
    "    if not getattr(tok, \"is_fast\", False):\n",
    "        raise RuntimeError(\"Fast tokenizer is required for offset_mapping.\")\n",
    "    return tok\n",
    "\n",
    "\n",
    "def load_base_model_eager(model_path: str):\n",
    "    disable_sdpa_flash()\n",
    "    try:\n",
    "        model = AutoModel.from_pretrained(\n",
    "            model_path,\n",
    "            trust_remote_code=True,\n",
    "            torch_dtype=dtype,\n",
    "            attn_implementation=\"eager\",\n",
    "        )\n",
    "    except TypeError:\n",
    "        model = AutoModel.from_pretrained(\n",
    "            model_path,\n",
    "            trust_remote_code=True,\n",
    "            torch_dtype=dtype,\n",
    "        )\n",
    "        for obj in [model, getattr(model, \"base_model\", None), getattr(model, \"model\", None)]:\n",
    "            if obj is None:\n",
    "                continue\n",
    "            try:\n",
    "                obj.set_attn_implementation(\"eager\")\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "\n",
    "def maybe_load_lora(model, adapter_path: str | None):\n",
    "    if adapter_path is None:\n",
    "        return model\n",
    "    if PeftModel is None:\n",
    "        raise RuntimeError(\"peft is not available. Install peft to load LoRA adapters.\")\n",
    "    model_peft = PeftModel.from_pretrained(model, adapter_path)\n",
    "    model_peft.to(device)\n",
    "    model_peft.eval()\n",
    "    return model_peft\n",
    "\n",
    "\n",
    "# =============================\n",
    "# Component parsing\n",
    "# =============================\n",
    "_OUT_PAT = re.compile(r\"(发病数|死亡数|cases|case|deaths|death)\", flags=re.IGNORECASE)\n",
    "\n",
    "\n",
    "def _trim_span(s: str, a: int, b: int) -> tuple[int, int]:\n",
    "    a = max(a, 0)\n",
    "    b = min(b, len(s))\n",
    "    while a < b and s[a] in \" \\t\\r\\n:：,，;；-—_()（）[]【】\":\n",
    "        a += 1\n",
    "    while b > a and s[b - 1] in \" \\t\\r\\n:：,，;；-—_()（）[]【】\":\n",
    "        b -= 1\n",
    "    return a, b\n",
    "\n",
    "\n",
    "def parse_components(prompt: str) -> dict[str, tuple[int, int]]:\n",
    "    \"\"\"\n",
    "    Return character spans for three components in the prompt:\n",
    "      - date    : last occurrence of YYYY年M月 or YYYY-M or YYYY/M\n",
    "      - outcome : last occurrence of outcome keywords\n",
    "      - disease : span between date end and outcome start\n",
    "    \"\"\"\n",
    "    s = prompt\n",
    "    L = len(s)\n",
    "\n",
    "    date_span = (0, 0)\n",
    "    date_end = None\n",
    "\n",
    "    mlist = list(re.finditer(r\"(?P<year>\\d{4})年(?P<month>\\d{1,2})月\", s))\n",
    "    if mlist:\n",
    "        m = mlist[-1]\n",
    "        date_span = (m.start(), m.end())\n",
    "        date_end = m.end()\n",
    "    else:\n",
    "        mlist2 = list(re.finditer(r\"(?P<year>\\d{4})\\s*[-/]\\s*(?P<month>\\d{1,2})\", s))\n",
    "        if mlist2:\n",
    "            m = mlist2[-1]\n",
    "            date_span = (m.start(), m.end())\n",
    "            date_end = m.end()\n",
    "\n",
    "    if date_end is None:\n",
    "        date_end = 0\n",
    "\n",
    "    out_span = (L, L)\n",
    "    om = list(_OUT_PAT.finditer(s))\n",
    "    if om:\n",
    "        m2 = om[-1]\n",
    "        out_span = (m2.start(), m2.end())\n",
    "\n",
    "    d0 = date_end\n",
    "    d1 = out_span[0] if out_span[0] > d0 else L\n",
    "    d0, d1 = _trim_span(s, d0, d1)\n",
    "\n",
    "    return {\"date\": date_span, \"disease\": (d0, d1), \"outcome\": out_span}\n",
    "\n",
    "\n",
    "def overlap_len(a: tuple[int, int], b: tuple[int, int]) -> int:\n",
    "    return max(0, min(a[1], b[1]) - max(a[0], b[0]))\n",
    "\n",
    "\n",
    "def assign_component(tok_span: tuple[int, int], comp_spans: dict[str, tuple[int, int]]) -> str:\n",
    "    best_k, best_ol = \"disease\", 0\n",
    "    for k, sp in comp_spans.items():\n",
    "        ol = overlap_len(tok_span, sp)\n",
    "        if ol > best_ol:\n",
    "            best_k, best_ol = k, ol\n",
    "    return best_k\n",
    "\n",
    "\n",
    "# =============================\n",
    "# Attention extraction + aggregation\n",
    "# =============================\n",
    "@torch.no_grad()\n",
    "def token_attention_profile(model, tokenizer, prompt: str, max_len: int = 128):\n",
    "    enc = tokenizer(\n",
    "        prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=max_len,\n",
    "        return_offsets_mapping=True,\n",
    "    )\n",
    "    if \"offset_mapping\" not in enc:\n",
    "        raise RuntimeError(\"offset_mapping missing (fast tokenizer required).\")\n",
    "\n",
    "    offsets = enc[\"offset_mapping\"][0].tolist()\n",
    "    input_ids = enc[\"input_ids\"].to(device)\n",
    "    attention_mask = enc[\"attention_mask\"].to(device)\n",
    "\n",
    "    out = model(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        output_attentions=True,\n",
    "        use_cache=False,\n",
    "        return_dict=True,\n",
    "    )\n",
    "    if out.attentions is None:\n",
    "        raise RuntimeError(\"attentions=None. Ensure eager attention (no sdpa/flash).\")\n",
    "\n",
    "    L = int(attention_mask[0].sum().item())\n",
    "    offsets = offsets[:L]\n",
    "\n",
    "    # Final layer, average across heads -> (L, L)\n",
    "    A = out.attentions[-1][0].float().mean(dim=0)[:L, :L]\n",
    "\n",
    "    if QUERY_MODE == \"last\":\n",
    "        prof = A[L - 1, :].float().cpu().numpy()\n",
    "    elif QUERY_MODE == \"lastk\":\n",
    "        k0 = max(0, L - int(LAST_K))\n",
    "        prof = A[k0:L, :].mean(dim=0).float().cpu().numpy()\n",
    "    elif QUERY_MODE == \"mean\":\n",
    "        prof = A.mean(dim=0).float().cpu().numpy()\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown QUERY_MODE={QUERY_MODE}\")\n",
    "\n",
    "    prof = np.maximum(prof, 0.0)\n",
    "    if prof.sum() > 0:\n",
    "        prof = prof / prof.sum()\n",
    "\n",
    "    return offsets, prof\n",
    "\n",
    "\n",
    "def aggregate_attention_to_components(prompt: str, offsets, attn_vec: np.ndarray) -> dict[str, float]:\n",
    "    comp_spans = parse_components(prompt)\n",
    "    weights = {\"date\": 0.0, \"disease\": 0.0, \"outcome\": 0.0}\n",
    "\n",
    "    for tok_span, w in zip(offsets, attn_vec):\n",
    "        a, b = int(tok_span[0]), int(tok_span[1])\n",
    "        if a == 0 and b == 0:\n",
    "            continue\n",
    "        if b <= a:\n",
    "            continue\n",
    "        comp = assign_component((a, b), comp_spans)\n",
    "        if comp in weights:\n",
    "            weights[comp] += float(w)\n",
    "\n",
    "    s = sum(weights.values())\n",
    "    if s > 0:\n",
    "        for k in weights:\n",
    "            weights[k] /= s\n",
    "    return weights\n",
    "\n",
    "\n",
    "def main():\n",
    "    tokenizer = load_tokenizer_fast_or_fail(BASE_MODEL_PATH)\n",
    "    base_model = load_base_model_eager(BASE_MODEL_PATH)\n",
    "    model = maybe_load_lora(base_model, LORA_ADAPTER_PATH)\n",
    "\n",
    "    for i, prompt in enumerate(DEMO_PROMPTS, start=1):\n",
    "        offsets, attn_prof = token_attention_profile(model, tokenizer, prompt, MAX_LEN)\n",
    "        comp_w = aggregate_attention_to_components(prompt, offsets, attn_prof)\n",
    "        print(f\"[{i}] prompt={prompt!r}\")\n",
    "        print(f\"    date={comp_w['date']:.6f}  disease={comp_w['disease']:.6f}  outcome={comp_w['outcome']:.6f}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
