{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a017f62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Zero-shot baseline (pretraining-contamination sensitivity analysis)\n",
    "\n",
    "What this script does\n",
    "---------------------\n",
    "This script implements a **zero-shot** baseline using a **frozen pretrained LLM**\n",
    "(e.g., Qwen-2.5-3B) with **NO task-specific fine-tuning**, to test whether near-correct\n",
    "test-period surveillance values could plausibly be explained by **memorization during\n",
    "pretraining**.\n",
    "\n",
    "Per disease–outcome task, it:\n",
    "  - sorts chronologically\n",
    "  - splits into train/val/test = 60%/20%/20%\n",
    "  - queries the pretrained-only model **ONLY on the test partition**\n",
    "  - enforces a strict prompt that requests a **one-line JSON** output: {\"value\": <integer>}\n",
    "  - decodes **generated tokens only** (to avoid parsing numbers from the prompt)\n",
    "  - extracts the first valid numeric value using a rule-based parser\n",
    "  - computes MAE/MSE on the **raw count scale**\n",
    "  - writes per-task Excel files + a dataset-level summary Excel\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# ================== Global config ==================\n",
    "random_seed = 3407\n",
    "cuda_device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "pretrained_path = os.environ.get(\"QWEN25_3B_PATH\", r\"E:\\Qwen2.5-3B\")\n",
    "\n",
    "# ---------- generation hyperparams ----------\n",
    "GEN_BATCH_SIZE = 8\n",
    "MAX_NEW_TOKENS = 24\n",
    "TEMPERATURE = 0.0\n",
    "TOP_P = 1.0\n",
    "NUM_BEAMS = 1\n",
    "\n",
    "# ---------- prompt template ----------\n",
    "\n",
    "PROMPT_TEMPLATE = (\n",
    "    \"你是一个数值回归器。请给出该指标对应的【数值】。\\n\"\n",
    "    \"输出必须严格为一行JSON：{{\\\"value\\\": <integer>}}\\n\"\n",
    "    \"要求：\\n\"\n",
    "    \"1) <integer> 必须是非负整数（不能有小数点）\\n\"\n",
    "    \"2) 不要输出年份、月份、日期，不要输出“年”“月”等字\\n\"\n",
    "    \"3) 除这一行JSON外不要输出任何其它文字\\n\"\n",
    "    \"指标：{instruction}\\n\"\n",
    "    \"输出：\"\n",
    ")\n",
    "\n",
    "# ---------- output dirs ----------\n",
    "OUT_ROOT = \"./zero_shot_outputs\"\n",
    "OUT_CHINA_DIR = os.path.join(OUT_ROOT, \"CHINA\")\n",
    "OUT_USAUS_DIR = os.path.join(OUT_ROOT, \"USAUS\")\n",
    "os.makedirs(OUT_CHINA_DIR, exist_ok=True)\n",
    "os.makedirs(OUT_USAUS_DIR, exist_ok=True)\n",
    "\n",
    "# ================== Utils ==================\n",
    "def set_all_seeds(seed: int = 3407):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def slugify(text):\n",
    "    s = str(text)\n",
    "    s = re.sub(r'[\\\\/:*?\"<>|]', \"_\", s)\n",
    "    s = re.sub(r\"\\s+\", \"_\", s.strip())\n",
    "    return s[:100]\n",
    "\n",
    "def extract_date_from_instruction(text: str):\n",
    "    \"\"\"Extract (year, month) from text like '2009年1月...发病数'.\"\"\"\n",
    "    m = re.search(r\"(\\d{4})年(\\d{1,2})月\", str(text))\n",
    "    if not m:\n",
    "        raise ValueError(f\"Date not found in text: {text}\")\n",
    "    return int(m.group(1)), int(m.group(2))\n",
    "\n",
    "def convert_date_to_ym(raw_date):\n",
    "    \"\"\"Convert Excel date to (year, month, date_str like '2009年1月').\"\"\"\n",
    "    if isinstance(raw_date, (int, float)):\n",
    "        if pd.isna(raw_date):\n",
    "            return (None, None, \"未知日期\")\n",
    "        base = datetime(1899, 12, 30)\n",
    "        real_date = base + timedelta(days=float(raw_date))\n",
    "        return (real_date.year, real_date.month, f\"{real_date.year}年{real_date.month}月\")\n",
    "    if isinstance(raw_date, datetime):\n",
    "        return (raw_date.year, raw_date.month, f\"{raw_date.year}年{raw_date.month}月\")\n",
    "    s = str(raw_date).strip()\n",
    "    if \"年\" in s and \"月\" in s:\n",
    "        y, m = extract_date_from_instruction(s + \"占位\")  # reuse regex\n",
    "        return (y, m, s)\n",
    "    try:\n",
    "        dt = pd.to_datetime(s)\n",
    "        return (dt.year, dt.month, f\"{dt.year}年{dt.month}月\")\n",
    "    except:\n",
    "        return (None, None, \"未知日期\")\n",
    "\n",
    "NUM_RE = r\"[-+]?\\d[\\d,]*\\.?\\d*(?:[eE][-+]?\\d+)?\"\n",
    "\n",
    "def _to_float(x: str):\n",
    "    try:\n",
    "        return float(x.replace(\",\", \"\"))\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "def parse_prediction(gen_text: str):\n",
    "    \"\"\"\n",
    "    Return: (pred_value, parse_rule)\n",
    "    Rules (in order):\n",
    "      1) JSON: {\"value\": number}\n",
    "      2) formatted_number / value = number\n",
    "      3) after 数值/预测值/预测：number\n",
    "      4) fallback: remove date like 2021年11月 then take LAST number\n",
    "    \"\"\"\n",
    "    if gen_text is None:\n",
    "        return np.nan, \"none\"\n",
    "\n",
    "    s = str(gen_text).strip()\n",
    "\n",
    "    # 1) JSON value\n",
    "    m = re.search(r'\"value\"\\s*:\\s*(' + NUM_RE + r')', s)\n",
    "    if m:\n",
    "        return _to_float(m.group(1)), \"json_value\"\n",
    "\n",
    "    # 2) formatted_number= / value=\n",
    "    m = re.search(r'(?:formatted_number|value)\\s*=\\s*\"?(' + NUM_RE + r')', s)\n",
    "    if m:\n",
    "        return _to_float(m.group(1)), \"key_equals\"\n",
    "\n",
    "    # 3) after 数值/预测值/预测:\n",
    "    m = re.search(r'(?:数值|预测值|预测)\\s*[:：]\\s*(' + NUM_RE + r')', s)\n",
    "    if m:\n",
    "        return _to_float(m.group(1)), \"after_colon\"\n",
    "\n",
    "    # 4) fallback: remove date patterns then take LAST number\n",
    "    s2 = re.sub(r'\\d{4}年\\d{1,2}月', ' ', s)   \n",
    "    s2 = re.sub(r'\\d{4}-\\d{1,2}', ' ', s2)    \n",
    "    nums = re.findall(NUM_RE, s2)\n",
    "    if nums:\n",
    "        return _to_float(nums[-1]), \"last_number_no_date\"\n",
    "\n",
    "    return np.nan, \"no_number\"\n",
    "\n",
    "# ================== Data loaders ==================\n",
    "def load_china_wide_to_long(excel_path: str):\n",
    "    \"\"\"\n",
    "    Read China wide Excel with columns: 指标, 日期, (disease cols...)\n",
    "    Output long df with columns:\n",
    "      - instruction, output, year, month, disease, measure\n",
    "    \"\"\"\n",
    "    df_excel = pd.read_excel(excel_path)\n",
    "    col_names = df_excel.columns.tolist()\n",
    "    disease_cols = col_names[2:]\n",
    "\n",
    "    rows = []\n",
    "    for _, row in df_excel.iterrows():\n",
    "        measure = str(row[\"指标\"]).strip()  # 发病数 / 死亡数\n",
    "        raw_date = row[\"日期\"]\n",
    "        year, month, date_str = convert_date_to_ym(raw_date)\n",
    "\n",
    "        for disease in disease_cols:\n",
    "            val = row[disease]\n",
    "            if pd.isna(val):\n",
    "                val = 0.0\n",
    "            disease_str = str(disease).strip()\n",
    "            instruction = f\"{date_str}{disease_str}{measure}\"\n",
    "            rows.append({\n",
    "                \"instruction\": instruction,\n",
    "                \"output\": float(val),\n",
    "                \"year\": year,\n",
    "                \"month\": month,\n",
    "                \"disease\": disease_str,\n",
    "                \"measure\": measure,\n",
    "            })\n",
    "    df = pd.DataFrame(rows)\n",
    "    # drop unknown-date rows if any\n",
    "    df = df.dropna(subset=[\"year\", \"month\"]).copy()\n",
    "    df[\"year\"] = df[\"year\"].astype(int)\n",
    "    df[\"month\"] = df[\"month\"].astype(int)\n",
    "    return df\n",
    "\n",
    "def load_usaus_long(excel_path: str):\n",
    "    \"\"\"\n",
    "    Read US/AUS long Excel which should already have:\n",
    "      - instruction, output, disease, year, month\n",
    "    \"\"\"\n",
    "    df = pd.read_excel(excel_path)\n",
    "    if \"output\" not in df.columns:\n",
    "        raise ValueError(\"US/AUS file must contain column 'output'.\")\n",
    "    df[\"output\"] = df[\"output\"].astype(float)\n",
    "\n",
    "    for c in [\"instruction\", \"disease\", \"year\", \"month\"]:\n",
    "        if c not in df.columns:\n",
    "            raise ValueError(f\"US/AUS file missing required column: {c}\")\n",
    "\n",
    "    df[\"year\"] = df[\"year\"].astype(int)\n",
    "    df[\"month\"] = df[\"month\"].astype(int)\n",
    "    return df\n",
    "\n",
    "# ================== Model (base zero-shot) ==================\n",
    "def load_base_llm(pretrained_path: str):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(pretrained_path, trust_remote_code=True)\n",
    "    # Decoder-only models: left padding is safer for batch generation\n",
    "    tokenizer.padding_side = \"left\"\n",
    "    if tokenizer.pad_token_id is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        pretrained_path,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map={\"\": 0} if torch.cuda.is_available() else None,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    model.eval()\n",
    "    return tokenizer, model\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_numbers(model, tokenizer, instructions, batch_size=8):\n",
    "    \"\"\"\n",
    "    Generate for a batch of instructions, return:\n",
    "      - gen_texts: generated-only strings\n",
    "      - preds: parsed floats (np.nan if parse fails)\n",
    "      - rules_all: which parser rule matched\n",
    "    \"\"\"\n",
    "    rules_all = []\n",
    "    prompts = [PROMPT_TEMPLATE.format(instruction=ins) for ins in instructions]\n",
    "\n",
    "    gen_texts_all = []\n",
    "    preds_all = []\n",
    "\n",
    "    for i in range(0, len(prompts), batch_size):\n",
    "        batch_prompts = prompts[i:i + batch_size]\n",
    "        enc = tokenizer(\n",
    "            batch_prompts,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=256\n",
    "        )\n",
    "        input_ids = enc[\"input_ids\"].to(cuda_device)\n",
    "        attention_mask = enc[\"attention_mask\"].to(cuda_device)\n",
    "\n",
    "        # per-sample prompt lengths (without padding)\n",
    "        prompt_lens = attention_mask.sum(dim=1).tolist()\n",
    "\n",
    "        gen_ids = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_new_tokens=MAX_NEW_TOKENS,\n",
    "            do_sample=False,\n",
    "            temperature=TEMPERATURE,\n",
    "            top_p=TOP_P,\n",
    "            num_beams=NUM_BEAMS,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "        # decode only the generated part (avoid parsing digits from prompt)\n",
    "        for j in range(gen_ids.size(0)):\n",
    "            full = gen_ids[j]\n",
    "            gen_part = full[prompt_lens[j]:]  # cut prompt tokens\n",
    "            gen_text = tokenizer.decode(gen_part, skip_special_tokens=True).strip()\n",
    "            gen_texts_all.append(gen_text)\n",
    "            val, rule = parse_prediction(gen_text)\n",
    "            preds_all.append(val)\n",
    "            rules_all.append(rule)\n",
    "\n",
    "    return gen_texts_all, np.array(preds_all, dtype=float), rules_all\n",
    "\n",
    "# ================== Core runner ==================\n",
    "def run_zero_shot_for_dataset(df, dataset_name, out_dir, tasks):\n",
    "    \"\"\"\n",
    "    df must contain:\n",
    "      - instruction, output, disease, year, month\n",
    "    tasks:\n",
    "      - for CHINA: [\"发病数\",\"死亡数\"] (stored in df['measure'])\n",
    "      - for USAUS: [\"发病数\"] (no measure column required; we just label it)\n",
    "    \"\"\"\n",
    "    tokenizer, model = load_base_llm(pretrained_path)\n",
    "\n",
    "    summary_rows = []\n",
    "    all_diseases = sorted(df[\"disease\"].unique().tolist())\n",
    "\n",
    "    for disease in all_diseases:\n",
    "        for task in tasks:\n",
    "            if dataset_name == \"CHINA\":\n",
    "                sub = df[(df[\"disease\"] == disease) & (df[\"measure\"] == task)].copy()\n",
    "            else:\n",
    "                # USAUS: single task label\n",
    "                sub = df[df[\"disease\"] == disease].copy()\n",
    "\n",
    "            if sub.empty:\n",
    "                continue\n",
    "\n",
    "            # sort by time\n",
    "            sub = sub.sort_values([\"year\", \"month\"]).reset_index(drop=True)\n",
    "\n",
    "            N = len(sub)\n",
    "            if N < 5:\n",
    "                continue\n",
    "\n",
    "            train_size = int(0.6 * N)\n",
    "            valid_size = int(0.2 * N)\n",
    "            test_size = N - train_size - valid_size\n",
    "\n",
    "            test_sub = sub.iloc[train_size + valid_size:].copy()\n",
    "            if test_sub.empty:\n",
    "                continue\n",
    "\n",
    "            test_instructions = test_sub[\"instruction\"].tolist()\n",
    "            test_actual = test_sub[\"output\"].to_numpy(dtype=float)\n",
    "            test_dates = [datetime(int(y), int(m), 1) for y, m in zip(test_sub[\"year\"], test_sub[\"month\"])]\n",
    "\n",
    "            # -------- zero-shot generate --------\n",
    "            gen_texts, preds, parse_rules = generate_numbers(\n",
    "                model, tokenizer, test_instructions, batch_size=GEN_BATCH_SIZE\n",
    "            )\n",
    "\n",
    "            # metrics on parsed subset\n",
    "            ok = ~np.isnan(preds)\n",
    "            n_test = len(preds)\n",
    "            n_ok = int(ok.sum())\n",
    "            parse_rate = n_ok / n_test if n_test > 0 else 0.0\n",
    "\n",
    "            if n_ok > 0:\n",
    "                mse = float(np.mean((preds[ok] - test_actual[ok]) ** 2))\n",
    "                mae = float(np.mean(np.abs(preds[ok] - test_actual[ok])))\n",
    "            else:\n",
    "                mse, mae = np.nan, np.nan\n",
    "\n",
    "            # save per-task details\n",
    "            safe_disease = slugify(disease)\n",
    "            safe_task = slugify(task)\n",
    "\n",
    "            detail_df = pd.DataFrame({\n",
    "                \"Date\": test_dates,\n",
    "                \"Instruction\": test_instructions,\n",
    "                \"Actual\": test_actual,\n",
    "                \"Generated_Text\": gen_texts,      # raw generated-only text\n",
    "                \"Predicted_Parsed\": preds,\n",
    "                \"Parse_rule\": parse_rules         # which parsing rule extracted the number\n",
    "            })\n",
    "\n",
    "            fn = (\n",
    "                f\"{safe_disease}_{safe_task}_zeroshoot.xlsx\"\n",
    "                if dataset_name == \"CHINA\"\n",
    "                else f\"{safe_disease}_zeroshoot.xlsx\"\n",
    "            )\n",
    "            detail_path = os.path.join(out_dir, fn)\n",
    "            detail_df.to_excel(detail_path, index=False)\n",
    "\n",
    "            summary_rows.append({\n",
    "                \"Dataset\": dataset_name,\n",
    "                \"Disease\": disease,\n",
    "                \"Task\": task,\n",
    "                \"N_total_test\": n_test,\n",
    "                \"N_parsed\": n_ok,\n",
    "                \"Parse_rate\": round(parse_rate, 4),\n",
    "                \"MSE\": mse,\n",
    "                \"MAE\": mae,\n",
    "                \"Detail_file\": detail_path\n",
    "            })\n",
    "\n",
    "            print(\n",
    "                f\"[{dataset_name}] {disease} {task} | test={n_test} parsed={n_ok} \"\n",
    "                f\"rate={parse_rate:.2%} | MSE={mse:.4f} MAE={mae:.4f} -> {detail_path}\"\n",
    "            )\n",
    "\n",
    "    summary_df = pd.DataFrame(summary_rows)\n",
    "\n",
    "    # overall aggregation\n",
    "    if not summary_df.empty:\n",
    "        overall = summary_df.copy()\n",
    "\n",
    "        overall_mse_mean = np.nanmean(overall[\"MSE\"].values) if overall[\"MSE\"].notna().any() else np.nan\n",
    "        overall_mae_mean = np.nanmean(overall[\"MAE\"].values) if overall[\"MAE\"].notna().any() else np.nan\n",
    "        overall_parse = np.nanmean(overall[\"Parse_rate\"].values) if overall[\"Parse_rate\"].notna().any() else np.nan\n",
    "\n",
    "        # weighted by N_parsed\n",
    "        w = overall[\"N_parsed\"].to_numpy(dtype=float)\n",
    "        mse_w = overall[\"MSE\"].to_numpy(dtype=float)\n",
    "        mae_w = overall[\"MAE\"].to_numpy(dtype=float)\n",
    "        if np.nansum(w) > 0:\n",
    "            overall_mse_w = np.nansum(mse_w * w) / np.nansum(w)\n",
    "            overall_mae_w = np.nansum(mae_w * w) / np.nansum(w)\n",
    "        else:\n",
    "            overall_mse_w, overall_mae_w = np.nan, np.nan\n",
    "\n",
    "        overall_row = pd.DataFrame([{\n",
    "            \"Dataset\": dataset_name,\n",
    "            \"Disease\": \"__OVERALL__\",\n",
    "            \"Task\": \"__OVERALL__\",\n",
    "            \"N_total_test\": int(overall[\"N_total_test\"].sum()),\n",
    "            \"N_parsed\": int(overall[\"N_parsed\"].sum()),\n",
    "            \"Parse_rate\": round(float(overall_parse), 4) if not np.isnan(overall_parse) else np.nan,\n",
    "            \"MSE\": overall_mse_mean,\n",
    "            \"MAE\": overall_mae_mean,\n",
    "            \"Detail_file\": \"\"\n",
    "        }])\n",
    "        summary_df = pd.concat([summary_df, overall_row], ignore_index=True)\n",
    "\n",
    "        overall_row_w = pd.DataFrame([{\n",
    "            \"Dataset\": dataset_name,\n",
    "            \"Disease\": \"__OVERALL_WEIGHTED__\",\n",
    "            \"Task\": \"__OVERALL_WEIGHTED__\",\n",
    "            \"N_total_test\": int(overall[\"N_total_test\"].sum()),\n",
    "            \"N_parsed\": int(overall[\"N_parsed\"].sum()),\n",
    "            \"Parse_rate\": round(float(overall_parse), 4) if not np.isnan(overall_parse) else np.nan,\n",
    "            \"MSE\": overall_mse_w,\n",
    "            \"MAE\": overall_mae_w,\n",
    "            \"Detail_file\": \"\"\n",
    "        }])\n",
    "        summary_df = pd.concat([summary_df, overall_row_w], ignore_index=True)\n",
    "\n",
    "    summary_path = os.path.join(out_dir, f\"{dataset_name}_zero_shot_summary.xlsx\")\n",
    "    summary_df.to_excel(summary_path, index=False)\n",
    "    print(f\"\\n[{dataset_name}] Summary saved -> {summary_path}\\n\")\n",
    "    return summary_df, summary_path\n",
    "\n",
    "# ================== MAIN ==================\n",
    "def main():\n",
    "    set_all_seeds(random_seed)\n",
    "\n",
    "    china_excel_path = \"example_china_wide.xlsx\"\n",
    "    usaus_excel_path = \"example_usaus_long.xlsx\"\n",
    "\n",
    "    # ---- CHINA ----\n",
    "    if os.path.exists(china_excel_path):\n",
    "        df_china = load_china_wide_to_long(china_excel_path)\n",
    "        # tasks in China file (e.g., 发病数 / 死亡数)\n",
    "        china_tasks = sorted(df_china[\"measure\"].unique().tolist())\n",
    "        run_zero_shot_for_dataset(\n",
    "            df=df_china,\n",
    "            dataset_name=\"CHINA\",\n",
    "            out_dir=OUT_CHINA_DIR,\n",
    "            tasks=china_tasks\n",
    "        )\n",
    "    else:\n",
    "        print(f\"[CHINA] File not found: {china_excel_path}\")\n",
    "\n",
    "    # ---- USAUS ----\n",
    "    if os.path.exists(usaus_excel_path):\n",
    "        df_usaus = load_usaus_long(usaus_excel_path)\n",
    "        # only one task label\n",
    "        usaus_tasks = [\"发病数\"]\n",
    "        run_zero_shot_for_dataset(\n",
    "            df=df_usaus,\n",
    "            dataset_name=\"USAUS\",\n",
    "            out_dir=OUT_USAUS_DIR,\n",
    "            tasks=usaus_tasks\n",
    "        )\n",
    "    else:\n",
    "        print(f\"[USAUS] File not found: {usaus_excel_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
