{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7d54f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "LLM-NID Forecasting · Minimal Reproducible Core\n",
    "================================================\n",
    "This script implements the *public* trunk of the infectious-disease forecasting pipeline.\n",
    "It fine-tunes a LoRA-adapted Qwen-2.5-3B model on a single (disease, outcome) time series and\n",
    "performs evaluation on a 60/20/20 split. Update path variables and hyperparameters to suit\n",
    "your environment.\n",
    "\"\"\"\n",
    "\n",
    "import os, random, json, re\n",
    "from copy import deepcopy\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    AutoModel,\n",
    "    AutoTokenizer,\n",
    "    get_linear_schedule_with_warmup,\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "# -------- 1. Configuration & Hyperparameters --------\n",
    "SEED             = 3407\n",
    "PRETRAINED_MODEL = os.getenv(\"QWEN_PATH\", \"./qwen-2_5-3b\")  # Absolute path or Hugging Face repo\n",
    "DATA_FILE        = \"./china_nid_2009_2025.xlsx\"              # Excel with columns: 指标, 日期, diseases...\n",
    "DEVICE           = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "MAX_EPOCHS       = 20\n",
    "BATCH_SIZE       = 16\n",
    "LEARNING_RATE    = 5e-5\n",
    "LOSS_ALPHA       = 10.0  # Weight factor for target-based loss scaling\n",
    "\n",
    "# -------- 2. Utility Functions --------\n",
    "DATE_PATTERN = re.compile(r\"(\\d{4})年(\\d{1,2})月\")\n",
    "\n",
    "def extract_date(text: str):\n",
    "    match = DATE_PATTERN.search(text)\n",
    "    if not match:\n",
    "        raise ValueError(f\"No date found in '{text}'\")\n",
    "    return int(match.group(1)), int(match.group(2))\n",
    "\n",
    "def excel_date_to_text(raw):\n",
    "    \"\"\"Convert raw Excel date (number/date/string) to 'YYYY年M月'.\"\"\"\n",
    "    if isinstance(raw, (int, float)):\n",
    "        base = datetime(1899, 12, 30)\n",
    "        real = base + timedelta(days=float(raw))\n",
    "    else:\n",
    "        real = pd.to_datetime(raw)\n",
    "    return f\"{real.year}年{real.month}月\"\n",
    "\n",
    "# -------- 3. Dataset Preparation --------\n",
    "def build_dataset(xlsx_path):\n",
    "    df_raw = pd.read_excel(xlsx_path)\n",
    "    disease_columns = df_raw.columns[2:]\n",
    "    records = []\n",
    "    for _, row in df_raw.iterrows():\n",
    "        measure = str(row[\"指标\"]).strip()\n",
    "        date_str = excel_date_to_text(row[\"日期\"])\n",
    "        for disease in disease_columns:\n",
    "            value = 0.0 if pd.isna(row[disease]) else float(row[disease])\n",
    "            records.append({\n",
    "                \"instruction\": f\"{date_str}{disease}{measure}\",\n",
    "                \"output\": value\n",
    "            })\n",
    "    df = pd.DataFrame(records)\n",
    "    df[\"output_log\"] = np.log1p(df.output)\n",
    "    return df\n",
    "\n",
    "# -------- 4. PyTorch Dataset & Collate Function --------\n",
    "class TSRegressionDataset(Dataset):\n",
    "    def __init__(self, data_tuples, tokenizer, max_length=128):\n",
    "        self.targets = [t[0] for t in data_tuples]\n",
    "        self.texts   = [t[1] for t in data_tuples]\n",
    "        self.enc     = tokenizer(self.texts, truncation=True, padding=True, max_length=max_length)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.targets)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {k: torch.tensor(v[idx]) for k, v in self.enc.items()}\n",
    "        item[\"targets\"] = torch.tensor(self.targets[idx], dtype=torch.float)\n",
    "        item[\"text\"]    = self.texts[idx]\n",
    "        return item\n",
    "\n",
    "def collate_batch(batch):\n",
    "    batched = {\n",
    "        \"input_ids\": torch.stack([b[\"input_ids\"] for b in batch]),\n",
    "        \"attention_mask\": torch.stack([b[\"attention_mask\"] for b in batch]),\n",
    "        \"targets\": torch.stack([b[\"targets\"] for b in batch])\n",
    "    }\n",
    "    batched[\"texts\"] = [b[\"text\"] for b in batch]\n",
    "    return batched\n",
    "\n",
    "# -------- 5. Model Definition: LoRA-Qwen + Time Embeddings --------\n",
    "def load_lora_qwen_model():\n",
    "    base_model = AutoModel.from_pretrained(\n",
    "        PRETRAINED_MODEL,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map={\"\": 0}\n",
    "    )\n",
    "    # Freeze all except embeddings and final two layers\n",
    "    for name, param in base_model.named_parameters():\n",
    "        param.requires_grad = any(\n",
    "            key in name for key in [\"embed_tokens\", \"layers.30\", \"layers.31\"]\n",
    "        )\n",
    "    peft_conf = LoraConfig(\n",
    "        r=8,\n",
    "        lora_alpha=64,\n",
    "        target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"down_proj\",\"up_proj\"],\n",
    "        lora_dropout=0.1,\n",
    "        bias=\"none\",\n",
    "        task_type=\"SEQ_CLS\"\n",
    "    )\n",
    "    return get_peft_model(base_model, peft_conf)\n",
    "\n",
    "class TimeSeriesLoRAModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.backbone = load_lora_qwen_model()\n",
    "        hidden = self.backbone.config.hidden_size\n",
    "        self.year0, self.n_year = 2004, 40\n",
    "        embed_dim = 128\n",
    "        self.year_emb  = nn.Embedding(self.n_year, embed_dim)\n",
    "        self.month_emb = nn.Embedding(12, embed_dim)\n",
    "        self.dropout   = nn.Dropout(0.4)\n",
    "        self.regressor = nn.Sequential(\n",
    "            nn.Linear(hidden + 2*embed_dim, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(hidden, 1)\n",
    "        )\n",
    "        self.loss_fn = nn.MSELoss(reduction=\"none\")\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, targets=None, texts=None):\n",
    "        out = self.backbone(input_ids=input_ids, attention_mask=attention_mask, return_dict=True)\n",
    "        mask = attention_mask.unsqueeze(-1).float()\n",
    "        pooled = (out.last_hidden_state * mask).sum(1) / mask.sum(1).clamp(min=1e-9)\n",
    "        if texts is not None:\n",
    "            years, months = zip(*(extract_date(t) for t in texts))\n",
    "            years = torch.tensor([y - self.year0 for y in years], dtype=torch.long, device=DEVICE)\n",
    "            months = torch.tensor([m-1 for m in months], dtype=torch.long, device=DEVICE)\n",
    "            ext = torch.cat([self.year_emb(years), self.month_emb(months)], dim=1)\n",
    "            pooled = torch.cat([pooled, ext], dim=1)\n",
    "        preds = self.regressor(self.dropout(pooled)).squeeze(-1)\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            weights = 1.0 + LOSS_ALPHA * targets\n",
    "            raw = self.loss_fn(preds, targets)\n",
    "            loss = (weights * raw).mean()\n",
    "        return preds, loss\n",
    "\n",
    "# -------- 6. Training and Evaluation Utilities --------\n",
    "def train_one_epoch(model, dataloader, optimizer, scaler, scheduler):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for batch in tqdm(dataloader, desc=\"Training\", leave=False):\n",
    "        optimizer.zero_grad()\n",
    "        with torch.cuda.amp.autocast(enabled=DEVICE.startswith(\"cuda\")):\n",
    "            preds, loss = model(\n",
    "                input_ids=batch[\"input_ids\"].to(DEVICE),\n",
    "                attention_mask=batch[\"attention_mask\"].to(DEVICE),\n",
    "                targets=batch[\"targets\"].to(DEVICE),\n",
    "                texts=batch[\"texts\"]\n",
    "            )\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        scheduler.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def evaluate_model(model, dataloader, data_scaler):\n",
    "    model.eval()\n",
    "    all_t, all_p, all_txt = [], [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\", leave=False):\n",
    "            preds, _ = model(\n",
    "                input_ids=batch[\"input_ids\"].to(DEVICE),\n",
    "                attention_mask=batch[\"attention_mask\"].to(DEVICE),\n",
    "                texts=batch[\"texts\"]\n",
    "            )\n",
    "            all_t.extend(batch[\"targets\"].cpu().numpy())\n",
    "            all_p.extend(preds.cpu().numpy())\n",
    "            all_txt.extend(batch[\"texts\"])\n",
    "    t_log = data_scaler.inverse_transform(np.array(all_t)[:,None]).flatten()\n",
    "    p_log = data_scaler.inverse_transform(np.array(all_p)[:,None]).flatten()\n",
    "    actual = np.expm1(t_log)\n",
    "    predicted = np.expm1(p_log).clip(min=0)\n",
    "    return {\n",
    "        \"mse\": mean_squared_error(actual, predicted),\n",
    "        \"mae\": mean_absolute_error(actual, predicted)\n",
    "    }, actual, predicted, all_txt\n",
    "\n",
    "# -------- 7. Single Series Demo --------\n",
    "def main():\n",
    "    random.seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    torch.manual_seed(SEED)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(PRETRAINED_MODEL, trust_remote_code=True)\n",
    "\n",
    "    df_all = build_dataset(DATA_FILE)\n",
    "    # >>> Customize this section for your disease and measure <<<\n",
    "    DISEASE = \"Influenza\"\n",
    "    OUTCOME = \"发病数\"\n",
    "    subset = df_all[df_all.instruction.str.contains(DISEASE) & df_all.instruction.str.contains(OUTCOME)]\n",
    "    subset = subset.sort_values(by=\"instruction\", key=lambda s: s.map(lambda x: extract_date(x)))\n",
    "    assert len(subset) > 10, \"Insufficient data for selected series\"\n",
    "\n",
    "    N = len(subset)\n",
    "    n_train = int(0.6 * N)\n",
    "    n_val   = int(0.2 * N)\n",
    "    train_df, val_df, test_df = subset[:n_train], subset[n_train:n_train+n_val], subset[n_train+n_val:]\n",
    "\n",
    "    # Build data loaders\n",
    "    scaler = MinMaxScaler().fit(train_df.output_log.values.reshape(-1,1))\n",
    "    def mk_loader(df):\n",
    "        scaled = scaler.transform(df.output_log.values.reshape(-1,1)).flatten()\n",
    "        data = list(zip(scaled, df.instruction.tolist()))\n",
    "        ds = TSRegressionDataset(data, tokenizer)\n",
    "        return DataLoader(ds, batch_size=BATCH_SIZE, shuffle=(df is train_df), collate_fn=collate_batch)\n",
    "    dl_train, dl_val, dl_test = mk_loader(train_df), mk_loader(val_df), mk_loader(test_df)\n",
    "\n",
    "    model = TimeSeriesLoRAModel().to(DEVICE)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=0.05)\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, 200, len(dl_train) * MAX_EPOCHS)\n",
    "    scaler_amp = torch.cuda.amp.GradScaler(enabled=DEVICE.startswith(\"cuda\"))\n",
    "\n",
    "    best_mse, patience = float('inf'), 0\n",
    "    for epoch in range(MAX_EPOCHS):\n",
    "        loss = train_one_epoch(model, dl_train, optimizer, scaler_amp, scheduler)\n",
    "        metrics_val, _, _, _ = evaluate_model(model, dl_val, scaler)\n",
    "        print(f\"Epoch {epoch+1}/{MAX_EPOCHS} - train_loss={loss:.4f}, val_mse={metrics_val['mse']:.4f}\")\n",
    "        if metrics_val['mse'] < best_mse:\n",
    "            best_mse, patience = metrics_val['mse'], 0\n",
    "            best_state = deepcopy(model.state_dict())\n",
    "        else:\n",
    "            patience += 1\n",
    "            if patience >= 3: break\n",
    "\n",
    "    model.load_state_dict(best_state)\n",
    "    metrics_test, y_true, y_pred, txt = evaluate_model(model, dl_test, scaler)\n",
    "    print(\"Test metrics:\", metrics_test)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
