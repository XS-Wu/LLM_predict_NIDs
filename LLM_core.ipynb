{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3bb0b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "LLM-NID Forecasting\n",
    "================================================\n",
    "This script implements the *public* trunk of the infectious-disease forecasting pipeline.\n",
    "It fine-tunes a LoRA-adapted Qwen-2.5-3B model on a single (disease, outcome) time series and\n",
    "performs evaluation on a 60/20/20 split. \n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from copy import deepcopy\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoModel, AutoTokenizer, get_linear_schedule_with_warmup\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from datetime import datetime, timedelta\n",
    "import re\n",
    "\n",
    "\n",
    "# ================== Hyperparameters  ==================\n",
    "random_seed = 3407\n",
    "\n",
    "pretrained_path = \"/path/to/Qwen2.5-3B\"\n",
    "\n",
    "dropout_rate = 0.2\n",
    "max_epoch_num = 20  # number of training epochs\n",
    "cuda_device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "early_stop_patience = 3\n",
    "best_valid_loss = float(\"inf\")\n",
    "no_improve_epochs = 0\n",
    "\n",
    "excel_path = os.path.join(\".\", \"data\", \"example_input.xlsx\")\n",
    "\n",
    "\n",
    "# ================== Step 1: Read and convert Excel to unified DataFrame ==================\n",
    "def process_excel_to_df(excel_file):\n",
    "    \"\"\"\n",
    "    Read Excel and convert it into a DataFrame with columns:\n",
    "      - instruction: e.g., \"2009年1月鼠疫发病数\"\n",
    "      - output: numeric target\n",
    "\n",
    "    Excel schema assumptions :\n",
    "      - First two columns are: 指标, 日期\n",
    "      - Disease columns start from the 3rd column\n",
    "    \"\"\"\n",
    "    df_excel = pd.read_excel(excel_file)\n",
    "    col_names = df_excel.columns.tolist()\n",
    "\n",
    "    # First two columns are [\"指标\", \"日期\"]; disease columns begin at the 3rd column\n",
    "    disease_cols = col_names[2:]\n",
    "\n",
    "    data_list = []\n",
    "    for _, row in df_excel.iterrows():\n",
    "        measure = str(row[\"指标\"]).strip()  # \"发病数\" / \"死亡数\"\n",
    "        raw_date = row[\"日期\"]\n",
    "\n",
    "        # Convert date to \"yyyy年m月\"\n",
    "        date_str = convert_date_to_ym_str(raw_date)\n",
    "\n",
    "        for disease in disease_cols:\n",
    "            value = row[disease]\n",
    "            if pd.isna(value):\n",
    "                value = 0.0  \n",
    "\n",
    "            disease_str = str(disease).strip()\n",
    "            # Concatenate into \"2009年1月鼠疫发病数\"\n",
    "            instruction = f\"{date_str}{disease_str}{measure}\"\n",
    "            data_list.append({\"instruction\": instruction, \"output\": value})\n",
    "\n",
    "    df_out = pd.DataFrame(data_list)\n",
    "    return df_out\n",
    "\n",
    "\n",
    "def convert_date_to_ym_str(raw_date):\n",
    "    \"\"\"\n",
    "    Convert an Excel date (could be numeric serial / datetime / string) to \"yyyy年m月\".\n",
    "    \"\"\"\n",
    "    if isinstance(raw_date, (int, float)):\n",
    "        if pd.isna(raw_date):\n",
    "            return \"UnknownDate\"\n",
    "        base = datetime(1899, 12, 30)\n",
    "        delta = timedelta(days=raw_date)\n",
    "        real_date = base + delta\n",
    "        return f\"{real_date.year}年{real_date.month}月\"\n",
    "    elif isinstance(raw_date, datetime):\n",
    "        return f\"{raw_date.year}年{raw_date.month}月\"\n",
    "    else:\n",
    "        s = str(raw_date).strip()\n",
    "        if \"年\" in s and \"月\" in s:\n",
    "            return s\n",
    "        try:\n",
    "            dt = pd.to_datetime(s)\n",
    "            return f\"{dt.year}年{dt.month}月\"\n",
    "        except Exception:\n",
    "            return \"UnknownDate\"\n",
    "\n",
    "\n",
    "# Read and convert into the unified format\n",
    "df = process_excel_to_df(excel_path)\n",
    "\n",
    "\n",
    "# ================== Helpers: parse date and disease name from instruction ==================\n",
    "def extract_date(text):\n",
    "    \"\"\"\n",
    "    Extract (year, month) from instruction.\n",
    "    Example: \"2009年1月鼠疫发病数\" -> (2009, 1)\n",
    "    \"\"\"\n",
    "    match = re.search(r\"(\\d{4})年(\\d{1,2})月\", text)\n",
    "    if match:\n",
    "        year = int(match.group(1))\n",
    "        month = int(match.group(2))\n",
    "        return year, month\n",
    "    raise ValueError(f\"Date pattern not found in instruction: {text}\")\n",
    "\n",
    "\n",
    "def extract_disease(instruction):\n",
    "    \"\"\"\n",
    "    Extract disease name from instruction.\n",
    "    Examples:\n",
    "      - \"2009年1月鼠疫发病数\" -> \"鼠疫\"\n",
    "      - \"2011年10月流行性感冒死亡数\" -> \"流行性感冒\"\n",
    "\n",
    "    If both patterns match, \"发病数\" is checked first.\n",
    "    \"\"\"\n",
    "    m = re.search(r\"\\d{4}年\\d{1,2}月(.*?)发病数\", instruction)\n",
    "    if m:\n",
    "        return m.group(1)\n",
    "    m = re.search(r\"\\d{4}年\\d{1,2}月(.*?)死亡数\", instruction)\n",
    "    if m:\n",
    "        return m.group(1)\n",
    "    return \"Unknown\"\n",
    "\n",
    "\n",
    "# Add log1p-transformed outputs \n",
    "df[\"output\"] = df[\"output\"].astype(float)\n",
    "df[\"output_log\"] = np.log1p(df[\"output\"])\n",
    "\n",
    "\n",
    "# ================== Dataset class and collate_fn ==================\n",
    "class RegressionDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_length=128):\n",
    "        self.targets = [x[0] for x in data] \n",
    "        self.texts = [x[1] for x in data] \n",
    "        self.encodings = tokenizer(\n",
    "            self.texts, truncation=True, padding=True, max_length=max_length\n",
    "        )\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item[\"targets\"] = torch.tensor(self.targets[idx], dtype=torch.float)\n",
    "        item[\"text\"] = self.texts[idx]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.targets)\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    batch_dict = {\n",
    "        \"input_ids\": torch.stack([item[\"input_ids\"] for item in batch]),\n",
    "        \"attention_mask\": torch.stack([item[\"attention_mask\"] for item in batch]),\n",
    "        \"targets\": torch.stack([item[\"targets\"] for item in batch]),\n",
    "    }\n",
    "    if \"text\" in batch[0]:\n",
    "        batch_dict[\"texts\"] = [item[\"text\"] for item in batch]\n",
    "    return batch_dict\n",
    "\n",
    "\n",
    "# ================== Model-related functions ==================\n",
    "def freeze_and_configure_base_model(base_model):\n",
    "    \"\"\"\n",
    "    Freeze all pretrained weights except:\n",
    "      - token embeddings (embed_tokens)\n",
    "      - the last two transformer layers: layers.30 and layers.31\n",
    "    \"\"\"\n",
    "    for name, param in base_model.named_parameters():\n",
    "        if \"embed_tokens\" in name or \"layers.30\" in name or \"layers.31\" in name:\n",
    "            param.requires_grad = True\n",
    "        else:\n",
    "            param.requires_grad = False\n",
    "\n",
    "\n",
    "def get_model():\n",
    "    base_model = AutoModel.from_pretrained(\n",
    "        pretrained_path,\n",
    "        device_map={\"\": 0},\n",
    "        torch_dtype=torch.float16,\n",
    "    )\n",
    "    freeze_and_configure_base_model(base_model)\n",
    "\n",
    "    peft_config = LoraConfig(\n",
    "        r=16,\n",
    "        lora_alpha=64,\n",
    "        target_modules=[\n",
    "            \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "            \"down_proj\", \"up_proj\"\n",
    "        ],\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "        task_type=\"SEQ_CLS\",\n",
    "    )\n",
    "    model = get_peft_model(base_model, peft_config)\n",
    "    return model\n",
    "\n",
    "\n",
    "class EnhancedLoRAModel(nn.Module):\n",
    "    def __init__(self, disease_list):\n",
    "\n",
    "        super().__init__()\n",
    "        self.lora_model = get_model()\n",
    "        hidden_size = self.lora_model.config.hidden_size\n",
    "\n",
    "        # Year/month embeddings\n",
    "        self.year_offset = 2004\n",
    "        self.max_years = 40\n",
    "        embed_dim = 128  # embedding dimension for temporal features\n",
    "        self.year_emb = nn.Embedding(self.max_years, embed_dim)\n",
    "        self.month_emb = nn.Embedding(12, embed_dim)\n",
    "\n",
    "        # Regressor input: hidden_size + 2 * embed_dim (year + month)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.regressor = nn.Sequential(\n",
    "            nn.Linear(hidden_size + embed_dim * 2, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(hidden_size, 1),\n",
    "        ).to(cuda_device)\n",
    "\n",
    "        self.mse_loss = nn.MSELoss(reduction=\"none\")\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, targets=None, texts=None):\n",
    "        outputs = self.lora_model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            return_dict=True,\n",
    "        )\n",
    "        last_hidden = outputs.last_hidden_state\n",
    "        input_mask = attention_mask.unsqueeze(-1).float()\n",
    "        pooled = (last_hidden * input_mask).sum(dim=1) / input_mask.sum(dim=1).clamp(min=1e-9)\n",
    "\n",
    "        # Add year/month embeddings only\n",
    "        if texts is not None:\n",
    "            years, months = [], []\n",
    "            for instr in texts:\n",
    "                y, m = extract_date(instr)\n",
    "                years.append(y)\n",
    "                months.append(m)\n",
    "\n",
    "            years = torch.tensor(\n",
    "                [y - self.year_offset for y in years],\n",
    "                dtype=torch.long,\n",
    "                device=cuda_device,\n",
    "            )\n",
    "            months = torch.tensor(\n",
    "                [m - 1 for m in months],\n",
    "                dtype=torch.long,\n",
    "                device=cuda_device,\n",
    "            )\n",
    "\n",
    "            year_vec = self.year_emb(years)\n",
    "            month_vec = self.month_emb(months)\n",
    "            extra_feat = torch.cat([year_vec, month_vec], dim=-1)\n",
    "            pooled = torch.cat([pooled, extra_feat], dim=-1)\n",
    "\n",
    "        pooled = self.dropout(pooled)\n",
    "        predictions = self.regressor(pooled).squeeze(-1)\n",
    "\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            alpha = 10.0\n",
    "            weight = 1.0 + alpha * targets\n",
    "            raw_loss = self.mse_loss(predictions, targets)\n",
    "            weighted_loss = (weight * raw_loss).mean()\n",
    "            loss = weighted_loss\n",
    "\n",
    "        return {\"loss\": loss, \"predictions\": predictions}\n",
    "\n",
    "\n",
    "# ================== Training & evaluation ==================\n",
    "def train_epoch(model, dataloader, optimizer, grad_scaler, scheduler):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for batch in tqdm(dataloader, desc=\"Training\", leave=False):\n",
    "        inputs = {\n",
    "            \"input_ids\": batch[\"input_ids\"].to(cuda_device),\n",
    "            \"attention_mask\": batch[\"attention_mask\"].to(cuda_device),\n",
    "            \"targets\": batch[\"targets\"].to(cuda_device),\n",
    "            \"texts\": batch[\"texts\"],\n",
    "        }\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # mixed-precision logic\n",
    "        with torch.autocast(device_type=\"cuda\", dtype=torch.float16):\n",
    "            outputs = model(**inputs)\n",
    "            loss = outputs[\"loss\"]\n",
    "\n",
    "        grad_scaler.scale(loss).backward()\n",
    "        trainable_params = (p for p in model.parameters() if p.requires_grad)\n",
    "        torch.nn.utils.clip_grad_norm_(trainable_params, max_norm=1.0)\n",
    "        grad_scaler.step(optimizer)\n",
    "        grad_scaler.update()\n",
    "        scheduler.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "\n",
    "def evaluate(model, dataloader, data_scaler):\n",
    "    model.eval()\n",
    "    all_targets = []\n",
    "    all_predictions = []\n",
    "    all_texts = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\", leave=False):\n",
    "            inputs = {\n",
    "                \"input_ids\": batch[\"input_ids\"].to(cuda_device),\n",
    "                \"attention_mask\": batch[\"attention_mask\"].to(cuda_device),\n",
    "                \"targets\": batch[\"targets\"].to(cuda_device),\n",
    "                \"texts\": batch[\"texts\"],\n",
    "            }\n",
    "            outputs = model(**inputs)\n",
    "            predictions = outputs[\"predictions\"]\n",
    "\n",
    "            all_targets.extend(inputs[\"targets\"].cpu().numpy())\n",
    "            all_predictions.extend(predictions.detach().cpu().numpy())\n",
    "            all_texts.extend(batch[\"texts\"])\n",
    "\n",
    "    # Inverse transform:\n",
    "    # 1) inverse MinMax to recover log-space values\n",
    "    # 2) expm1 to recover original scale\n",
    "    all_targets_log = data_scaler.inverse_transform(np.array(all_targets).reshape(-1, 1)).flatten()\n",
    "    all_predictions_log = data_scaler.inverse_transform(np.array(all_predictions).reshape(-1, 1)).flatten()\n",
    "    all_predictions_log = np.maximum(all_predictions_log, 0)\n",
    "\n",
    "    all_targets_actual = np.expm1(all_targets_log)\n",
    "    all_predictions_actual = np.expm1(all_predictions_log)\n",
    "\n",
    "    mse = mean_squared_error(all_targets_actual, all_predictions_actual)\n",
    "    mae = mean_absolute_error(all_targets_actual, all_predictions_actual)\n",
    "\n",
    "    metrics = {\"mse\": mse, \"mae\": mae}\n",
    "    return metrics, all_targets_actual, all_predictions_actual, all_texts\n",
    "\n",
    "\n",
    "# ================== Main ==================\n",
    "def main():\n",
    "    random.seed(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "    torch.manual_seed(random_seed)\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(pretrained_path, trust_remote_code=True)\n",
    "\n",
    "    # Collect all disease names (even though disease embeddings are not used,\n",
    "    # we keep this to construct per-disease tasks exactly as the original code)\n",
    "    all_diseases = sorted(df[\"instruction\"].apply(extract_disease).unique().tolist())\n",
    "\n",
    "    # Detect which tasks exist in the dataset \n",
    "    all_tasks = []\n",
    "    for v in [\"发病数\", \"死亡数\"]:\n",
    "        if any(v in ins for ins in df[\"instruction\"]):\n",
    "            all_tasks.append(v)\n",
    "\n",
    "    for disease in all_diseases:\n",
    "        for task in all_tasks:\n",
    "            print(f\"\\nStart training: Disease={disease} | Task={task}\")\n",
    "\n",
    "            # Filter subset for this disease + task\n",
    "            sub_df = df[df[\"instruction\"].apply(lambda x: (disease in x) and (task in x))]\n",
    "            if sub_df.empty:\n",
    "                print(f\"No data, skip: {disease} | {task}\")\n",
    "                continue\n",
    "\n",
    "            # Sort by date\n",
    "            def safe_extract_date(instr):\n",
    "                try:\n",
    "                    return extract_date(instr)\n",
    "                except Exception:\n",
    "                    return (9999, 12)\n",
    "\n",
    "            sub_df = sub_df.copy()\n",
    "            sub_df[\"sort_key\"] = sub_df[\"instruction\"].apply(safe_extract_date)\n",
    "            sub_df = sub_df.sort_values(\"sort_key\").reset_index(drop=True)\n",
    "            sub_df.drop(columns=[\"sort_key\"], inplace=True)\n",
    "\n",
    "            N = len(sub_df)\n",
    "            if N < 5:\n",
    "                print(f\"Too few data points, skip: {disease} | {task}\")\n",
    "                continue\n",
    "\n",
    "            # Split 60/20/20 by time order \n",
    "            train_size = int(0.6 * N)\n",
    "            valid_size = int(0.2 * N)\n",
    "            test_size = N - train_size - valid_size\n",
    "\n",
    "            train_sub = sub_df.iloc[:train_size]\n",
    "            valid_sub = sub_df.iloc[train_size : train_size + valid_size]\n",
    "            test_sub = sub_df.iloc[train_size + valid_size :]\n",
    "\n",
    "            # Fit scaler on train only \n",
    "            scaler = MinMaxScaler()\n",
    "            scaler.fit(train_sub[\"output_log\"].values.reshape(-1, 1))\n",
    "\n",
    "            train_targets = scaler.transform(train_sub[\"output_log\"].values.reshape(-1, 1)).flatten()\n",
    "            valid_targets = scaler.transform(valid_sub[\"output_log\"].values.reshape(-1, 1)).flatten()\n",
    "            test_targets = scaler.transform(test_sub[\"output_log\"].values.reshape(-1, 1)).flatten()\n",
    "\n",
    "            # Build (target, instruction) lists \n",
    "            train_data_list = list(zip(train_targets, train_sub[\"instruction\"]))\n",
    "            valid_data_list = list(zip(valid_targets, valid_sub[\"instruction\"]))\n",
    "            test_data_list = list(zip(test_targets, test_sub[\"instruction\"]))\n",
    "\n",
    "            max_length = 128\n",
    "            train_dataset = RegressionDataset(train_data_list, tokenizer, max_length=max_length)\n",
    "            valid_dataset = RegressionDataset(valid_data_list, tokenizer, max_length=max_length)\n",
    "            test_dataset = RegressionDataset(test_data_list, tokenizer, max_length=max_length)\n",
    "\n",
    "            train_dataloader = DataLoader(\n",
    "                train_dataset, batch_size=16, shuffle=True, collate_fn=collate_fn, num_workers=0\n",
    "            )\n",
    "            valid_dataloader = DataLoader(\n",
    "                valid_dataset, batch_size=16, shuffle=False, collate_fn=collate_fn, num_workers=0\n",
    "            )\n",
    "            test_dataloader = DataLoader(\n",
    "                test_dataset, batch_size=16, shuffle=False, collate_fn=collate_fn, num_workers=0\n",
    "            )\n",
    "\n",
    "            # Train model for this disease (disease_list kept for signature compatibility)\n",
    "            model = EnhancedLoRAModel(disease_list=[disease]).to(cuda_device)\n",
    "            trainable_params = [p for p in model.parameters() if p.requires_grad]\n",
    "            optimizer = torch.optim.AdamW(trainable_params, lr=5e-5, weight_decay=0.05)\n",
    "\n",
    "            total_steps = len(train_dataloader) * max_epoch_num\n",
    "            warmup_steps = min(200, max(1, int(0.1 * total_steps)))\n",
    "            scheduler = get_linear_schedule_with_warmup(\n",
    "                optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps\n",
    "            )\n",
    "\n",
    "            grad_scaler = torch.amp.GradScaler(\"cuda\")\n",
    "\n",
    "            best_valid_mse = float(\"inf\")\n",
    "            best_model_state = None\n",
    "            no_improve_epochs = 0\n",
    "\n",
    "            # Training loop with early stopping on valid MSE \n",
    "            for epoch in range(max_epoch_num):\n",
    "                print(f\"Epoch {epoch + 1}/{max_epoch_num}\")\n",
    "                train_loss = train_epoch(model, train_dataloader, optimizer, grad_scaler, scheduler)\n",
    "                valid_metrics, _, _, _ = evaluate(model, valid_dataloader, scaler)\n",
    "                print(\n",
    "                    f\"TrainLoss: {train_loss:.4f} | \"\n",
    "                    f\"ValidMSE: {valid_metrics['mse']:.4f} | ValidMAE: {valid_metrics['mae']:.4f}\"\n",
    "                )\n",
    "\n",
    "                if valid_metrics[\"mse\"] < best_valid_mse:\n",
    "                    best_valid_mse = valid_metrics[\"mse\"]\n",
    "                    best_model_state = deepcopy(model.state_dict())\n",
    "                    no_improve_epochs = 0\n",
    "                else:\n",
    "                    no_improve_epochs += 1\n",
    "                    if no_improve_epochs >= early_stop_patience:\n",
    "                        print(\"Early stopping triggered.\")\n",
    "                        break\n",
    "\n",
    "            # Load best model state\n",
    "            if best_model_state is not None:\n",
    "                model.load_state_dict(best_model_state)\n",
    "\n",
    "            # Test evaluation and save predictions to Excel \n",
    "            test_metrics, test_actual, test_pred, test_texts = evaluate(model, test_dataloader, scaler)\n",
    "\n",
    "            test_dates = []\n",
    "            for instr in test_texts:\n",
    "                y, m = extract_date(instr)\n",
    "                test_dates.append(datetime(y, m, 1))\n",
    "\n",
    "            test_result_df = pd.DataFrame(\n",
    "                {\n",
    "                    \"Date\": test_dates,\n",
    "                    \"Instruction\": test_texts,\n",
    "                    \"Actual\": test_actual,\n",
    "                    \"Predicted\": test_pred,\n",
    "                }\n",
    "            )\n",
    "\n",
    "            results_dir = \"./results\"\n",
    "            os.makedirs(results_dir, exist_ok=True)\n",
    "            result_file = os.path.join(results_dir, f\"{disease}_{task}.xlsx\")\n",
    "            test_result_df.to_excel(result_file, index=False)\n",
    "            print(f\"Test results saved to: {result_file}\")\n",
    "            print(f\"Test metrics: MSE={test_metrics['mse']:.6f}, MAE={test_metrics['mae']:.6f}\")\n",
    "\n",
    "            # Save model and related artifacts \n",
    "            save_dir = os.path.join(\"./models\", f\"{disease}_{task}\")\n",
    "            os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "            tokenizer.save_pretrained(save_dir)\n",
    "            model.lora_model.save_pretrained(os.path.join(save_dir, \"lora_adapters\"))\n",
    "            torch.save(model.regressor.state_dict(), os.path.join(save_dir, \"regressor.pth\"))\n",
    "\n",
    "            scaler_params = {\"min_\": scaler.min_.tolist(), \"scale_\": scaler.scale_.tolist()}\n",
    "            with open(os.path.join(save_dir, \"scaler_params.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(scaler_params, f, ensure_ascii=False)\n",
    "\n",
    "            print(f\"Model saved to: {save_dir}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
